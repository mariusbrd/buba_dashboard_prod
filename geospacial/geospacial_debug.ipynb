{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9845ec",
   "metadata": {},
   "source": [
    "### Verarbeitung der Daten: Deutschlandatlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf4a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] geschrieben: c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 19\\geospacial\\data_input\\deutschlandatlas_processed\\Deutschlandatlas_GEM_merged.parquet\n",
      "[OK] geschrieben: c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 19\\geospacial\\data_input\\deutschlandatlas_processed\\Deutschlandatlas_KRS_merged.parquet\n",
      "[OK] geschrieben: c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 19\\geospacial\\data_input\\deutschlandatlas_processed\\Deutschlandatlas_VBGEM_merged.parquet\n",
      "Fertig. Dateien liegen in: c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 19\\geospacial\\data_input\\deutschlandatlas_processed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================\n",
    "# Projektbasis ermitteln (funktioniert im .py und halbwegs im Notebook)\n",
    "# ============================================================\n",
    "try:\n",
    "    # wenn als .py ausgeführt\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # wenn im Notebook ausgeführt\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "# jetzt alles relativ zu BASE_DIR\n",
    "DATA_INPUT_DIR = BASE_DIR / \"data_input\"\n",
    "\n",
    "# Pfad zur großen Ursprungsdatei (relativ)\n",
    "SRC_PATH = DATA_INPUT_DIR / \"Deutschlandatlas.xlsx\"\n",
    "\n",
    "# Ausgabeordner (relativ)\n",
    "OUTPUT_DIR = DATA_INPUT_DIR / \"deutschlandatlas_processed\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "# ============================================================\n",
    "\n",
    "# Präfixe + gewünschte Namensspalte pro Datei\n",
    "PREFIX_CONFIG = {\n",
    "    \"Deutschlandatlas_GEM\": \"Gemeindename\",\n",
    "    \"Deutschlandatlas_KRS\": \"Kreisname\",\n",
    "    \"Deutschlandatlas_VBGEM\": \"Gemeindeverbandsname\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_gkz(val, width: int = 8) -> str:\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    s = str(val).strip()\n",
    "    try:\n",
    "        num = int(float(s.replace(\" \", \"\")))\n",
    "        return str(num).zfill(width)\n",
    "    except (ValueError, TypeError):\n",
    "        if \".\" in s:\n",
    "            s = s.split(\".\")[0]\n",
    "        s = s.replace(\" \", \"\")\n",
    "        return s.zfill(width)\n",
    "\n",
    "\n",
    "def longest_name_per_gkz(df_names: pd.DataFrame, name_col: str) -> pd.DataFrame:\n",
    "    df = df_names.copy()\n",
    "    df[\"__len__\"] = df[name_col].fillna(\"\").astype(str).str.len()\n",
    "    df = (\n",
    "        df.sort_values([\"GKZ\", \"__len__\"], ascending=[True, False])\n",
    "          .drop_duplicates(subset=[\"GKZ\"], keep=\"first\")\n",
    "          .drop(columns=[\"__len__\"])\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_vbgem_gkz_canonical_map(all_names: pd.DataFrame, name_col: str) -> dict:\n",
    "    fix_map = {}\n",
    "    for name, grp in all_names.groupby(name_col):\n",
    "        gkz_list = [g for g in grp[\"GKZ\"].dropna().unique()]\n",
    "        if not gkz_list:\n",
    "            continue\n",
    "        zero_gkz = [g for g in gkz_list if g.startswith(\"0\")]\n",
    "        if zero_gkz:\n",
    "            canonical = min(zero_gkz)\n",
    "        else:\n",
    "            canonical = min(gkz_list)\n",
    "        for g in gkz_list:\n",
    "            fix_map[g] = canonical\n",
    "    return fix_map\n",
    "\n",
    "\n",
    "def main():\n",
    "    # komplette Excel einlesen\n",
    "    xls = pd.ExcelFile(SRC_PATH)\n",
    "\n",
    "    # Sheets nach Präfix sammeln\n",
    "    grouped = {p: [] for p in PREFIX_CONFIG.keys()}\n",
    "    for sheet in xls.sheet_names:\n",
    "        for prefix in PREFIX_CONFIG.keys():\n",
    "            if sheet.startswith(prefix):\n",
    "                df = pd.read_excel(xls, sheet_name=sheet)\n",
    "                grouped[prefix].append((sheet, df))\n",
    "                break\n",
    "\n",
    "    # jetzt je Präfix abarbeiten\n",
    "    for prefix, sheets in grouped.items():\n",
    "        if not sheets:\n",
    "            continue\n",
    "\n",
    "        target_name_col = PREFIX_CONFIG[prefix]\n",
    "\n",
    "        name_frames = []\n",
    "        dataframes_for_merge = []\n",
    "\n",
    "        for sheet_name, df in sheets:\n",
    "            if df.shape[1] < 1:\n",
    "                continue\n",
    "\n",
    "            # erste Spalte = GKZ\n",
    "            gkz_col = df.columns[0]\n",
    "            df[gkz_col] = df[gkz_col].apply(normalize_gkz)\n",
    "\n",
    "            # zweite Spalte = Name einsammeln\n",
    "            if df.shape[1] >= 2:\n",
    "                current_name_col = df.columns[1]\n",
    "                tmp = df[[gkz_col, current_name_col]].copy()\n",
    "                tmp = tmp.rename(columns={gkz_col: \"GKZ\", current_name_col: target_name_col})\n",
    "                name_frames.append(tmp)\n",
    "\n",
    "            # GKZ vereinheitlichen\n",
    "            df = df.rename(columns={gkz_col: \"GKZ\"})\n",
    "\n",
    "            # Namen aus diesem Sheet entfernen – später sauber wieder mergen\n",
    "            if df.shape[1] >= 2:\n",
    "                second_col = df.columns[1]\n",
    "                if second_col != \"GKZ\":\n",
    "                    df = df.drop(columns=[second_col], errors=\"ignore\")\n",
    "\n",
    "            dataframes_for_merge.append(df)\n",
    "\n",
    "        # VBGEM-Sonderfall\n",
    "        if prefix == \"Deutschlandatlas_VBGEM\" and name_frames:\n",
    "            all_names_raw = pd.concat(name_frames, ignore_index=True)\n",
    "            fix_map = build_vbgem_gkz_canonical_map(all_names_raw, target_name_col)\n",
    "\n",
    "            fixed_name_frames = []\n",
    "            for nf in name_frames:\n",
    "                nf = nf.copy()\n",
    "                nf[\"GKZ\"] = nf[\"GKZ\"].map(fix_map).fillna(nf[\"GKZ\"])\n",
    "                fixed_name_frames.append(nf)\n",
    "            name_frames = fixed_name_frames\n",
    "\n",
    "            fixed_dataframes_for_merge = []\n",
    "            for df in dataframes_for_merge:\n",
    "                df = df.copy()\n",
    "                df[\"GKZ\"] = df[\"GKZ\"].map(fix_map).fillna(df[\"GKZ\"])\n",
    "                fixed_dataframes_for_merge.append(df)\n",
    "            dataframes_for_merge = fixed_dataframes_for_merge\n",
    "\n",
    "        # doppelte Spaltennamen behandeln\n",
    "        level = prefix.rsplit(\"_\", 1)[-1]  # \"GEM\", \"KRS\", \"VBGEM\"\n",
    "\n",
    "        all_cols = []\n",
    "        for df in dataframes_for_merge:\n",
    "            all_cols.extend([c for c in df.columns if c != \"GKZ\"])\n",
    "\n",
    "        counts = Counter(all_cols)\n",
    "        duplicate_cols = {col for col, cnt in counts.items() if cnt > 1}\n",
    "\n",
    "        if duplicate_cols:\n",
    "            fixed_dfs = []\n",
    "            for df in dataframes_for_merge:\n",
    "                rename_dict = {c: f\"{level}_{c}\" for c in df.columns if c in duplicate_cols}\n",
    "                df = df.rename(columns=rename_dict)\n",
    "                fixed_dfs.append(df)\n",
    "            dataframes_for_merge = fixed_dfs\n",
    "\n",
    "        # Namen bereinigen\n",
    "        if name_frames:\n",
    "            all_names = pd.concat(name_frames, ignore_index=True)\n",
    "            all_names = all_names[all_names[target_name_col].notna() & (all_names[target_name_col] != \"\")]\n",
    "            unique_names = longest_name_per_gkz(all_names, target_name_col)\n",
    "        else:\n",
    "            unique_names = pd.DataFrame(columns=[\"GKZ\", target_name_col])\n",
    "\n",
    "        # alle Sheets über GKZ mergen\n",
    "        merged = None\n",
    "        for df in dataframes_for_merge:\n",
    "            if merged is None:\n",
    "                merged = df\n",
    "            else:\n",
    "                merged = pd.merge(merged, df, on=\"GKZ\", how=\"outer\")\n",
    "\n",
    "        # Namen zurückmergen\n",
    "        final_df = pd.merge(unique_names, merged, on=\"GKZ\", how=\"right\")\n",
    "\n",
    "        # GKZ säubern + sortieren\n",
    "        final_df[\"GKZ\"] = final_df[\"GKZ\"].apply(lambda x: normalize_gkz(x, 8)).astype(str)\n",
    "        final_df = final_df.sort_values([\"GKZ\"])\n",
    "\n",
    "        # relativ speichern\n",
    "        out_parquet = OUTPUT_DIR / f\"{prefix}_merged.parquet\"\n",
    "        final_df.to_parquet(out_parquet, index=False)\n",
    "        print(f\"[OK] geschrieben: {out_parquet}\")\n",
    "\n",
    "    print(\"Fertig. Dateien liegen in:\", OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61f1f1",
   "metadata": {},
   "source": [
    "### Erstellung der Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dd7f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[labels] Lade Indikator-Labels aus c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 17\\geospacial\\data_input\\deutschlandatlas_services_mit_counts.csv\n",
      "[labels] Fertig, 102 Label-Einträge.\n",
      "[meta] Lade Indikator-Metadaten aus c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 17\\geospacial\\data_input\\deutschlandatlas_indikatoren.csv\n",
      "[meta] Fertig, 86 Metadaten-Einträge.\n",
      "\n",
      "Welche Ebene willst du mappen?\n",
      "  1 = Kreise (KRS)\n",
      "  2 = Gemeinden (GEM)\n",
      "  3 = Gemeindeverband (VBGEM)\n",
      "→ [KRS] erkannte Schlüsselspalte: GKZ\n",
      "\n",
      "Verfügbare Spalten in Deutschlandatlas_KRS_merged.parquet:\n",
      "  1: GKZ\n",
      "  2: Kreisname\n",
      "  3: fl_suv – Anteil der Siedlungs- und Verkehrsfläche an der Gesamtfläche im Jahr 2020 in %\n",
      "  4: fl_landw – Anteil der Landwirtschaftsfläche an der Gesamtfläche im Jahr 2020 in %\n",
      "  5: fl_wald – Anteil der Waldfläche an der Gesamtfläche im Jahr 2020 in %\n",
      "  6: bev_binw – Saldo der Binnenwanderungen pro 10.000 Einwohner/-innen im Jahr 2020\n",
      "  7: bev_ausw – Saldo der Außenwanderungen pro 10.000 Einwohner/-innen im Jahr 2020\n",
      "  8: ko_kasskred – Kommunale Kassenkredite je Einwohner/-in im Jahr 2020 in €\n",
      "  9: bev_u18 – Anteil der unter 18-Jährigen an der Gesamtbevölkerung im Jahr 2019 in %\n",
      "  10: bev_18_65 – Anteil der 18- bis unter 65-Jährigen an der Gesamtbevölkerung im Jahr 2019 in %\n",
      "  11: bev_ue65 – Anteil der 65-Jährigen und Älteren an der Gesamtbevölkerung im Jahr 2019 in %\n",
      "  12: bev_ausl – Anteil der Ausländer/-innen an der Gesamtbevölkerung im Jahr 2019 in %\n",
      "  13: mitgl_sportv\n",
      "  14: ew_sportv\n",
      "  15: wahl_beteil – Bundestagswahlbeteiligung 2017 in Gemeinden in %\n",
      "  16: preis_miet – Wiedervermietungsmieten (Angebotsmieten nettokalt) im Jahr 2020 in € je m²\n",
      "  17: wohn_eigen\n",
      "  18: wohn_leer – Anteil leer stehender Wohnungen an allen Wohnungen im Jahr 2018 in %\n",
      "  19: wohn_EZFH – Fertiggestellte Wohnungen in neuen Ein- und Zweifamilienhäusern je 10.000 Einwohner-/innen im Jahr 2020\n",
      "  20: wohn_MFH – Fertiggestellte Wohnungen in neuen Mehrfamilienhäusern je 10.000 Einwohner-/innen im Jahr 2020\n",
      "  21: heiz_wohn_best\n",
      "  22: heiz_wohn – Anteil fertiggestellter Wohnungen mit primär erneuerbarer Heizenergie an allen errichteten Wohnungen in neuen Wohngebäuden im Jahr 2020 in %\n",
      "  23: bquali_unifh – Anteil sozialversicherungspflichtig Beschäftigter am Arbeitsort mit einem akademischen Abschluss an allen sozialversicherungspflichtig Beschäftigten im Jahr 2020 in %\n",
      "  24: bquali_mabschl – Anteil sozialversicherungspflichtig Beschäftigter am Arbeitsort mit einem anerkannten Berufsabschluss und ohne einen akademischen Abschluss an allen sozialversicherungspflichtig Beschäftigten im Jahr 2020 in %\n",
      "  25: bquali_oabschl – Anteil sozialversicherungspflichtig Beschäftigter am Arbeitsort ohne einen Berufs-/ akademischen Abschluss an allen sozialversicherungspflichtig Beschäftigten im Jahr 2020 in %\n",
      "  26: erw_wachs – Gemittelte Entwicklung der Erwerbstätigenzahl am Arbeitsort von 2010 bis 2020 pro Jahr in %\n",
      "  27: erw_vol – Veränderung des Arbeitsvolumens am Arbeitsort 2014 zu 2020 in %\n",
      "  28: erw_mini – Anteil der ausschließlich geringfügig entlohnten Beschäftigten am Arbeitsort an allen Erwerbstätigen im Jahr 2019 in %\n",
      "  29: erw_minineben – Anteil der geringfügig entlohnten Beschäftigten im Nebenjob am Arbeitsort an allen Erwerbstätigen im Jahr 2020 in %\n",
      "  30: teilz_insg – Anteil der sozialversicherungspflichtig Beschäftigten in Teilzeit am Arbeitsort an den sozialversicherungspflichtig Beschäftigten im Jahr 2021 in %\n",
      "  31: teilz_w – Weibliche sozialversicherungspflichtig Beschäftigte in Teilzeit am Arbeitsort an den weiblichen sozialversicherungspflichtig Beschäftigten im Jahr 2022 in %\n",
      "  32: teilz_m – Männliche sozialversicherungspflichtig Beschäftigte in Teilzeit am Arbeitsort an den männlichen sozialversicherungspflichtig Beschäftigten im Jahr 2021 in %\n",
      "  33: erw_bip – Bruttoinlandsprodukt je erwerbstätige Person im Jahr 2018 in 1.000 €\n",
      "  34: alq – Arbeitslosenquote bezogen auf alle zivilen Erwerbspersonen im Jahr 2020 in %\n",
      "  35: hh_veink – Verfügbares Einkommen privater Haushalte je Einwohner/-in im Jahr 2019 in 1.000 €\n",
      "  36: elterng_v – Anteil der Kinder, deren Vater Elterngeld bezogen hat, an allen anspruchsbegründeten Kindern im Jahr 2018 in %\n",
      "  37: schulden – Anteil überschuldeter Personen über 18 Jahre an der Altersgruppe im Jahr 2022 in %\n",
      "  38: grusi_insg\n",
      "  39: grusi_w\n",
      "  40: grusi_m\n",
      "  41: sozsich – Anteil der Personen in sozialer Mindestsicherung an allen Einwohnerinnen und Einwohnern im Jahr 2020 in %\n",
      "  42: auto\n",
      "  43: elade – Öffentlich zugängliche Ladepunkte für Elektrofahrzeuge im Jahr 2021 je 100.000 Einwohner/-innen\n",
      "  44: eauto – Anteil von Pkw mit reinem Elektroantrieb (BEV) an allen Pkw im Jahr 2022 in %\n",
      "  45: v_harzt – Hausärzte/-ärztinnen je 100.000 Einwohner/-innen im Jahr 2020\n",
      "  46: v_karzt\n",
      "  47: schule_oabschl – Anteil der Schulabgänger/-innen ohne Hauptschulabschluss an allen Schulabgänger/-innen allgemeinbildender Schulen im Jahr 2020 in %\n",
      "  48: kbetr_u3 – Anteil der betreuten Kinder unter 3 Jahren in Kindertageseinrichtungen/-tagespflege an der Altersgruppe im Jahr 2020 in %\n",
      "  49: kbetr_ue3 – Anteil der betreuten Kinder ab 3 bis unter 6 Jahren in Kindertageseinrichtungen/-tagespflege an der Altersgruppe im Jahr 2020 in %\n",
      "  50: kbetr_ue6 – Anteil der betreuten Kinder ab 6 bis unter 11 Jahren in Kindertageseinrichtungen/-tagespflege an der Altersgruppe im Jahr 2020 in %\n",
      "  51: kbtr_pers – Plätze in Kindertageseinrichtungen je pädagogisch tätige Person im Jahr 2019\n",
      "  52: kinder_bg – Anteil der unter 15-Jährigen in SGB-II-Bedarfsgemeinschaften an der Altersgruppe im Jahr 2020 in %\n",
      "  53: straft – Straftaten insgesamt pro 100.000 Einwohner/-innen im Jahr 2020\n",
      "  54: einbr – Fälle von Wohnungseinbruchdiebstahl pro 100.000 Einwohner/-innen im Jahr 2021\n",
      "  55: preis_baul – Baulandpreise für Eigenheime im Jahr 2020 in € je m²\n",
      "  56: pfl_ambu – Anteil der Pflegebedürftigen in ambulanter Pflege an den Pflegebedürftigen insgesamt im Jahr 2019 in %\n",
      "  57: pfl_stat – Anteil der Pflegebedürftigen in stationärer Pflege an den Pflegebedürftigen insgesamt im Jahr 2019 in %\n",
      "  58: pfl_geld – Anteil der Pflegegeldempfänger/-innen im Jahr 2019 an den Pflegebedürftigen insgesamt in %\n",
      "→ [KRS] nehme Spalte: fl_suv\n",
      "[labels/meta] Mapping gefunden: 'fl_suv' -> 'Anteil der Siedlungs- und Verkehrsfläche an der Gesamtfläche im Jahr 2020 in %'\n",
      "[KRS] 400 Features gemerged.\n",
      "[KRS] GeoJSON geschrieben: c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 17\\geospacial\\data_output\\output_map.geojson\n",
      "[OK] HTML gespeichert: c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 17\\geospacial\\data_output\\output_map.html\n",
      "[KRS] Tabellen-CSV geschrieben: c:\\Users\\marius.brede\\Nextcloud\\tmp\\dev\\Zeitreihenanalyse - Dash Dashboard\\Version 17\\geospacial\\data_output\\output_map_data.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "import branca.colormap as cm\n",
    "\n",
    "# ============================================================\n",
    "# Basis ermitteln (Notebook oder .py)\n",
    "# ============================================================\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "DATA_INPUT_DIR = BASE_DIR / \"data_input\"\n",
    "\n",
    "# ============================================================\n",
    "# BASIS-PFADE (relativ)\n",
    "# ============================================================\n",
    "# GeoJSON / Shapes\n",
    "BASE_KRS = DATA_INPUT_DIR / \"Shapefiles - KRS\"\n",
    "BASE_GEM = DATA_INPUT_DIR / \"Shapefiles - GEM\"\n",
    "BASE_VBGEM = DATA_INPUT_DIR / \"Shapefiles - VBGEM\"\n",
    "\n",
    "# parquet-Dateien aus dem Aufbereitungsscript\n",
    "PROCESSED_DIR = DATA_INPUT_DIR / \"deutschlandatlas_processed\"\n",
    "\n",
    "# Label / Meta\n",
    "INDICATOR_CSV_PATH = DATA_INPUT_DIR / \"deutschlandatlas_services_mit_counts.csv\"\n",
    "INDICATOR_META_PATH = DATA_INPUT_DIR / \"deutschlandatlas_indikatoren.csv\"\n",
    "\n",
    "# Output (relativ)\n",
    "OUTPUT_MAP_DIR = BASE_DIR / \"data_output\"\n",
    "OUTPUT_MAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEBUG = True\n",
    "FAST_MODE = True\n",
    "\n",
    "# ============================================================\n",
    "# Konstanten\n",
    "# ============================================================\n",
    "MISSING_VALUES = frozenset({\n",
    "    None,\n",
    "    \"\",\n",
    "    \"nan\",\n",
    "    \"-9999\", \"-9999.0\", \"-9999,0\",\n",
    "    \"-99999\", \"-99999.0\", \"-99999,0\",\n",
    "    \"-999999\", \"-999999.0\", \"-999999,0\",\n",
    "})\n",
    "SUFFIX_DROP = frozenset({\"ha2023\", \"ha2022\", \"ha2021\", \"ha2020\", \"za2023\", \"za2022\", \"3857\"})\n",
    "INDICATOR_PREFIXES = frozenset({\"p\", \"v\", \"pendel\", \"teilz\", \"bev\", \"beschq\", \"kbetr\"})\n",
    "KEY_COLUMN_CANDIDATES = (\"GKZ\", \"gkz\", \"Gebietskennziffer\", \"gebietskennziffer\", \"KRS\", \"AGS\", \"ags\", \"VBG\", \"vbg\")\n",
    "GKZ_PROPERTIES = (\"GKZ\", \"Gebietskennziffer\", \"VBG\")\n",
    "NAME_FIELD_CANDIDATES = (\"Gemeindename\", \"Gemeindeverbandsname\", \"Kreisname\", \"NAME\", \"GEN\")\n",
    "\n",
    "\n",
    "def log(*args):\n",
    "    if DEBUG:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def normalize_gkz(val, width=8) -> str:\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return str(int(float(s.replace(\" \", \"\")))).zfill(width)\n",
    "    except (ValueError, AttributeError):\n",
    "        return s.split(\".\")[0].replace(\" \", \"\").zfill(width)\n",
    "\n",
    "\n",
    "def is_missing(val) -> bool:\n",
    "    if val is None:\n",
    "        return True\n",
    "    s = str(val).strip().lower()\n",
    "    return s in MISSING_VALUES\n",
    "\n",
    "\n",
    "def detect_key_col(columns):\n",
    "    for candidate in KEY_COLUMN_CANDIDATES:\n",
    "        if candidate in columns:\n",
    "            return candidate\n",
    "    return columns[0]\n",
    "\n",
    "\n",
    "def _strip_suffixes(parts: list[str]) -> list[str]:\n",
    "    while parts and parts[-1] in SUFFIX_DROP:\n",
    "        parts.pop()\n",
    "    return parts\n",
    "\n",
    "\n",
    "def canonicalize_indicator_name(name: str) -> str:\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    parts = str(name).strip().lower().split(\"_\")\n",
    "    parts = _strip_suffixes(parts)\n",
    "    if not parts:\n",
    "        return \"\"\n",
    "    if len(parts) >= 2 and parts[0] in INDICATOR_PREFIXES:\n",
    "        return f\"{parts[0]}_{parts[1]}\"\n",
    "    return \"_\".join(parts)\n",
    "\n",
    "\n",
    "def extract_long_from_layers(layers_val: str) -> str | None:\n",
    "    if not isinstance(layers_val, str) or not layers_val:\n",
    "        return None\n",
    "    txt = layers_val.strip()\n",
    "    if \": \" in txt:\n",
    "        txt = txt.split(\": \", 1)[1]\n",
    "    if \" (Features:\" in txt:\n",
    "        txt = txt.split(\" (Features:\", 1)[0]\n",
    "    return txt.strip() or None\n",
    "\n",
    "\n",
    "def load_indicator_label_map(csv_path: Path) -> dict[str, str]:\n",
    "    log(f\"[labels] Lade Indikator-Labels aus {csv_path}\")\n",
    "    if not csv_path.exists():\n",
    "        log(\"[labels] Datei existiert nicht – Label-Mapping wird leer.\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, dtype=str)\n",
    "    except Exception as e:\n",
    "        log(f\"[labels] CSV konnte nicht gelesen werden: {e}\")\n",
    "        return {}\n",
    "\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    title_col = cols_lower.get(\"title\")\n",
    "    layers_col = cols_lower.get(\"layers\")\n",
    "    tf_col = cols_lower.get(\"totalfeatures\")\n",
    "\n",
    "    if not title_col:\n",
    "        log(\"[labels] Keine 'title'-Spalte gefunden – Mapping bleibt leer.\")\n",
    "        return {}\n",
    "\n",
    "    mapping: dict[str, dict] = {}\n",
    "    for _, row in df.iterrows():\n",
    "        short_raw = str(row.get(title_col, \"\")).strip()\n",
    "        if not short_raw:\n",
    "            continue\n",
    "\n",
    "        canon = canonicalize_indicator_name(short_raw)\n",
    "        long_name = extract_long_from_layers(row.get(layers_col, \"\")) if layers_col else None\n",
    "        long_name = long_name or short_raw\n",
    "\n",
    "        score = 0\n",
    "        if tf_col:\n",
    "            try:\n",
    "                score = int(float(str(row.get(tf_col, \"0\")).replace(\",\", \".\")))\n",
    "            except (ValueError, AttributeError):\n",
    "                pass\n",
    "\n",
    "        if canon not in mapping or score > mapping[canon][\"score\"]:\n",
    "            mapping[canon] = {\"long\": long_name, \"score\": score}\n",
    "\n",
    "    final_map = {k: v[\"long\"] for k, v in mapping.items()}\n",
    "    log(f\"[labels] Fertig, {len(final_map)} Label-Einträge.\")\n",
    "    return final_map\n",
    "\n",
    "\n",
    "def load_indicator_meta(csv_path: Path) -> dict[str, dict]:\n",
    "    log(f\"[meta] Lade Indikator-Metadaten aus {csv_path}\")\n",
    "    if not csv_path.exists():\n",
    "        log(\"[meta] Datei existiert nicht – Metadaten bleiben leer.\")\n",
    "        return {}\n",
    "\n",
    "    df = pd.read_csv(csv_path, dtype=str)\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    code_col = cols_lower.get(\"code\") or cols_lower.get(\"indikator_code\") or cols_lower.get(\"indicator_code\") or list(df.columns)[0]\n",
    "    name_col = cols_lower.get(\"name\") or cols_lower.get(\"indikator_name\") or cols_lower.get(\"langname\") or None\n",
    "    desc_col = cols_lower.get(\"beschreibung\") or cols_lower.get(\"description\") or None\n",
    "\n",
    "    meta: dict[str, dict] = {}\n",
    "    for _, row in df.iterrows():\n",
    "        raw_code = str(row.get(code_col, \"\")).strip()\n",
    "        if not raw_code:\n",
    "            continue\n",
    "        canon = canonicalize_indicator_name(raw_code)\n",
    "        meta_entry = {\n",
    "            \"name\": str(row.get(name_col, \"\")).strip() if name_col else \"\",\n",
    "            \"desc\": str(row.get(desc_col, \"\")).strip() if desc_col else \"\",\n",
    "            \"raw\": raw_code,\n",
    "        }\n",
    "        meta[canon] = meta_entry\n",
    "\n",
    "    log(f\"[meta] Fertig, {len(meta)} Metadaten-Einträge.\")\n",
    "    return meta\n",
    "\n",
    "\n",
    "def build_map(geojson_data: dict, value_field: str, display_name: str | None = None):\n",
    "    feats = geojson_data.get(\"features\", [])\n",
    "    if not feats:\n",
    "        log(\"[WARNUNG] Keine Features zum Visualisieren vorhanden.\")\n",
    "        return\n",
    "\n",
    "    m = folium.Map(location=[51.163, 10.447], zoom_start=6, tiles=\"cartodbpositron\")\n",
    "\n",
    "    base_props = feats[0][\"properties\"]\n",
    "    tooltip_fields = []\n",
    "    for cand in (\"GKZ\", \"Gemeindename\", \"Gemeindeverbandsname\", \"Kreisname\", \"indicator_name\", value_field):\n",
    "        if cand in base_props:\n",
    "            tooltip_fields.append(cand)\n",
    "\n",
    "    cleaned_vals = []\n",
    "    for f in feats:\n",
    "        raw_v = f[\"properties\"].get(value_field)\n",
    "        if is_missing(raw_v):\n",
    "            continue\n",
    "        try:\n",
    "            cleaned_vals.append(float(str(raw_v).replace(\",\", \".\")))\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "\n",
    "    vmin, vmax = (min(cleaned_vals), max(cleaned_vals)) if cleaned_vals else (0, 1)\n",
    "\n",
    "    legend_caption = display_name or value_field\n",
    "    colormap = cm.linear.YlOrRd_09.scale(vmin, vmax)\n",
    "    colormap.caption = legend_caption\n",
    "\n",
    "    def style_function(feature):\n",
    "        v = feature[\"properties\"].get(value_field)\n",
    "        if is_missing(v):\n",
    "            return {\n",
    "                \"fillColor\": \"#cccccc\",\n",
    "                \"color\": \"black\",\n",
    "                \"weight\": 0.3,\n",
    "                \"fillOpacity\": 0.7,\n",
    "            }\n",
    "        try:\n",
    "            vv = float(str(v).replace(\",\", \".\"))\n",
    "            color = colormap(vv)\n",
    "        except (ValueError, TypeError):\n",
    "            color = \"#cccccc\"\n",
    "        return {\n",
    "            \"fillColor\": color,\n",
    "            \"color\": \"black\",\n",
    "            \"weight\": 0.3,\n",
    "            \"fillOpacity\": 0.7,\n",
    "        }\n",
    "\n",
    "    folium.GeoJson(\n",
    "        geojson_data,\n",
    "        name=legend_caption,\n",
    "        style_function=style_function,\n",
    "        tooltip=folium.GeoJsonTooltip(fields=tooltip_fields) if tooltip_fields else None,\n",
    "    ).add_to(m)\n",
    "\n",
    "    colormap.add_to(m)\n",
    "    folium.LayerControl().add_to(m)\n",
    "\n",
    "    out_html = OUTPUT_MAP_DIR / \"output_map.html\"\n",
    "    m.save(str(out_html))\n",
    "    log(f\"[OK] HTML gespeichert: {out_html}\")\n",
    "\n",
    "\n",
    "def process_one(base_dir: Path,\n",
    "                prefix: str,\n",
    "                indicator_labels: dict[str, str],\n",
    "                indicator_meta: dict[str, dict],\n",
    "                parquet_path: Path):\n",
    "    geojson_path = base_dir / f\"{prefix}_Map.geojson\"\n",
    "\n",
    "    with open(geojson_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gj = json.load(f)\n",
    "\n",
    "    # GKZ im GeoJSON normalisieren\n",
    "    for feat in gj.get(\"features\", []):\n",
    "        props = feat.get(\"properties\", {})\n",
    "        for gkz_prop in GKZ_PROPERTIES:\n",
    "            if gkz_prop in props:\n",
    "                props[\"GKZ\"] = normalize_gkz(props[gkz_prop])\n",
    "                break\n",
    "\n",
    "    # parquet laden\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    df = df.astype(str).replace({\"nan\": None})\n",
    "\n",
    "    key_col = detect_key_col(df.columns)\n",
    "    print(f\"→ [{prefix}] erkannte Schlüsselspalte: {key_col}\")\n",
    "\n",
    "    df[\"GKZ\"] = df[key_col].apply(normalize_gkz)\n",
    "\n",
    "    # Raw-Index aus Meta\n",
    "    meta_by_raw = {v[\"raw\"].lower(): v for v in indicator_meta.values() if v.get(\"raw\")}\n",
    "\n",
    "    print(f\"\\nVerfügbare Spalten in {parquet_path.name}:\")\n",
    "    for i, c in enumerate(df.columns, 1):\n",
    "        if c in (\"GKZ\", \"Gemeindename\", \"Kreisname\", \"Gemeindeverbandsname\"):\n",
    "            print(f\"  {i}: {c}\")\n",
    "            continue\n",
    "\n",
    "        meta_entry = meta_by_raw.get(c.lower())\n",
    "        if not meta_entry:\n",
    "            canon = canonicalize_indicator_name(c)\n",
    "            meta_entry = indicator_meta.get(canon)\n",
    "\n",
    "        if meta_entry and meta_entry.get(\"name\"):\n",
    "            long_label_for_display = meta_entry[\"name\"]\n",
    "        else:\n",
    "            canon = canonicalize_indicator_name(c)\n",
    "            long_label_for_display = indicator_labels.get(canon, \"\")\n",
    "\n",
    "        if long_label_for_display:\n",
    "            print(f\"  {i}: {c} – {long_label_for_display}\")\n",
    "        else:\n",
    "            print(f\"  {i}: {c}\")\n",
    "\n",
    "    user_in = input(f\"\\n[{prefix}] Spalte wählen (Nummer oder Name): \").strip()\n",
    "    value_col = df.columns[int(user_in) - 1] if user_in.isdigit() else user_in\n",
    "    print(f\"→ [{prefix}] nehme Spalte: {value_col}\")\n",
    "\n",
    "    # Meta für gewählten Indikator\n",
    "    meta_entry = meta_by_raw.get(value_col.lower())\n",
    "    if not meta_entry:\n",
    "        canon = canonicalize_indicator_name(value_col)\n",
    "        meta_entry = indicator_meta.get(canon, {})\n",
    "\n",
    "    canon = canonicalize_indicator_name(value_col)\n",
    "    long_label = indicator_labels.get(canon)\n",
    "    if meta_entry.get(\"name\"):\n",
    "        long_label = meta_entry[\"name\"]\n",
    "    indicator_desc = meta_entry.get(\"desc\", \"\")\n",
    "\n",
    "    if long_label:\n",
    "        log(f\"[labels/meta] Mapping gefunden: '{value_col}' -> '{long_label}'\")\n",
    "    else:\n",
    "        long_label = value_col\n",
    "\n",
    "    val_map = dict(zip(df[\"GKZ\"], df[value_col]))\n",
    "    name_field = next((cand for cand in NAME_FIELD_CANDIDATES if cand in df.columns), None)\n",
    "\n",
    "    merged = 0\n",
    "    tab_rows = []\n",
    "\n",
    "    for feat in gj.get(\"features\", []):\n",
    "        props = feat.get(\"properties\", {})\n",
    "        gkz = props.get(\"GKZ\")\n",
    "        if not gkz:\n",
    "            continue\n",
    "\n",
    "        v = val_map.get(gkz)\n",
    "        row = {\"GKZ\": gkz, \"indicator_code\": value_col, \"indicator_name\": long_label}\n",
    "\n",
    "        if name_field:\n",
    "            name_vals = df.loc[df[\"GKZ\"] == gkz, name_field]\n",
    "            if not name_vals.empty:\n",
    "                row[\"name\"] = name_vals.iloc[0]\n",
    "            else:\n",
    "                row[\"name\"] = props.get(name_field, \"\")\n",
    "        else:\n",
    "            row[\"name\"] = props.get(\"Gemeindename\") or props.get(\"Kreisname\") or props.get(\"Gemeindeverbandsname\") or \"\"\n",
    "\n",
    "        if indicator_desc:\n",
    "            row[\"indicator_desc\"] = indicator_desc\n",
    "\n",
    "        if v is None or is_missing(v):\n",
    "            props[\"indicator_code\"] = value_col\n",
    "            props[\"indicator_name\"] = long_label\n",
    "            if indicator_desc:\n",
    "                props[\"indicator_desc\"] = indicator_desc\n",
    "\n",
    "            row[value_col] = None\n",
    "            tab_rows.append(row)\n",
    "            continue\n",
    "\n",
    "        keep = {\"GKZ\": gkz, value_col: v, \"indicator_code\": value_col, \"indicator_name\": long_label}\n",
    "        if indicator_desc:\n",
    "            keep[\"indicator_desc\"] = indicator_desc\n",
    "\n",
    "        if name_field:\n",
    "            name_vals = df.loc[df[\"GKZ\"] == gkz, name_field]\n",
    "            if not name_vals.empty:\n",
    "                keep[name_field] = name_vals.iloc[0]\n",
    "\n",
    "        feat[\"properties\"] = keep\n",
    "        merged += 1\n",
    "\n",
    "        row[value_col] = v\n",
    "        tab_rows.append(row)\n",
    "\n",
    "    print(f\"[{prefix}] {merged} Features gemerged.\")\n",
    "\n",
    "    # GeoJSON immer in Output\n",
    "    out_geo = OUTPUT_MAP_DIR / \"output_map.geojson\"\n",
    "    with open(out_geo, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(gj, f, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "    print(f\"[{prefix}] GeoJSON geschrieben: {out_geo}\")\n",
    "\n",
    "    # HTML immer in Output\n",
    "    build_map(gj, value_col, display_name=long_label)\n",
    "\n",
    "    # Tabellen-CSV immer in Output\n",
    "    out_table = OUTPUT_MAP_DIR / \"output_map_data.csv\"\n",
    "    df_out = pd.DataFrame(tab_rows)\n",
    "    cols_order = [\"GKZ\", \"name\", value_col, \"indicator_code\", \"indicator_name\", \"indicator_desc\"]\n",
    "    df_out = df_out.reindex(columns=cols_order)\n",
    "    df_out.to_csv(out_table, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[{prefix}] Tabellen-CSV geschrieben: {out_table}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    indicator_labels = load_indicator_label_map(INDICATOR_CSV_PATH)\n",
    "    indicator_meta = load_indicator_meta(INDICATOR_META_PATH)\n",
    "\n",
    "    print(\"\\nWelche Ebene willst du mappen?\")\n",
    "    print(\"  1 = Kreise (KRS)\")\n",
    "    print(\"  2 = Gemeinden (GEM)\")\n",
    "    print(\"  3 = Gemeindeverband (VBGEM)\")\n",
    "\n",
    "    choice = input(\"\\nAuswahl: \").strip()\n",
    "\n",
    "    mapping = {\n",
    "        \"1\": (BASE_KRS, \"KRS\", PROCESSED_DIR / \"Deutschlandatlas_KRS_merged.parquet\"),\n",
    "        \"2\": (BASE_GEM, \"GEM\", PROCESSED_DIR / \"Deutschlandatlas_GEM_merged.parquet\"),\n",
    "        \"3\": (BASE_VBGEM, \"VBGEM\", PROCESSED_DIR / \"Deutschlandatlas_VBGEM_merged.parquet\"),\n",
    "    }\n",
    "\n",
    "    if choice in mapping:\n",
    "        base_dir, prefix, parquet_path = mapping[choice]\n",
    "        if not parquet_path.exists():\n",
    "            raise SystemExit(f\"Parquet nicht gefunden: {parquet_path}\")\n",
    "        process_one(base_dir, prefix, indicator_labels, indicator_meta, parquet_path)\n",
    "    else:\n",
    "        raise SystemExit(\"Ungültige Auswahl.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
